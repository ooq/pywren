import time
import boto3 
import uuid
import numpy as np
import time
import pywren
import subprocess
import logging
import sys
import boto3
import hashlib
import cPickle as pickle
import uuid
import click
# this is in general a bad idea, but oh well. 
import sys
sys.path.append("../")
import exampleutils
import botocore
import md5
from multiprocessing.pool import ThreadPool
import random

@click.group()
def cli():
    pass


def write(bucket_name, mb_per_file, number, key_prefix, 
          region):
    def run_command(mykey):
        #client = boto3.client("s3")
        bytes_n = mb_per_file
        print("bytes_n " + str(bytes_n))
        d = exampleutils.RandomDataGenerator(bytes_n).read(bytes_n)
        t1 = time.time()
        #results = {}
        ever_fail = False
        num_items = 1000
        def work(x):
            client = boto3.client("s3")
            for i in range(0,num_items):
            #for i in range(0,1):
                #print("x = " + str(x) + " i = " + str(i))
                key = random.randint(0,9999)
                keyname = "input/part-" + str(key)
                m = md5.new()
                m.update(keyname)
                randomized_keyname = "input/" + m.hexdigest()[:8] + "-part-" + str(key)
                try:
                    #data = client.get_object(Bucket = bucket_name, Key = randomized_keyname)['Body'].read()
                    client.put_object(Bucket = bucket_name, Key = randomized_keyname, Body=d)
                    #results[key] = len(data)
                except botocore.exceptions.ClientError as e:
                    #results[key] = False
                    t2 = time.time()
                    return t1, t2, 0
                    ever_fail = True
        poolsize = 5
        pool = ThreadPool(poolsize)
        print("work start")
        pool.map(work, range(mykey, mykey+poolsize))
        print("work end")
        pool.close()
        pool.join()
        t2 = time.time()
        if ever_fail:
            return t1, t2, 0        
        return t1, t2, num_items*poolsize/(t2-t1)

    # create list of random keys
    all_numbers = range(0, 100000, 10)[0:number]
    all_keys = [i%10000 for i in all_numbers]
    import random
    random.shuffle(all_keys)
    keynames = list(all_keys)
    print("start")
    #print(run_command(keynames[0]))
    #exit(0)
    wrenexec = pywren.default_executor(shard_runtime=True)
    futures = wrenexec.map_sync_with_rate_and_retries(run_command, keynames, straggler=True, WAIT_DUR_SEC=1, rate=10000)
    
    pywren.wait(futures) 
    results = [f.result() for f in futures]
    run_statuses = [f.run_status for f in futures]
    invoke_statuses = [f.invoke_status for f in futures]

    n_fail = len([r for r in results if r[2] == 0])
    add_tpt = sum([r[2] for r in results])
    print("n_fail: " + str(n_fail) + "  tpt: " + str(add_tpt))


    res = {'results' : results, 
           'run_statuses' : run_statuses, 
           'bucket_name' : bucket_name, 
           'keynames' : keynames, 
           'invoke_statuses' : invoke_statuses}
    return res


def read(bucket_name, number, 
         keylist_raw, read_times, region):
    
    blocksize = 1024*1024

    def run_command(key):
        t1 = time.time()
        time.sleep(10) 
        t2 = time.time()

        return t1, t2, 1

    wrenexec = pywren.default_executor(shard_runtime=True)
    if number == 0:
        keylist = keylist_raw
    else:
        keylist = [keylist_raw[i % len(keylist_raw)]  for i in range(number)]
    
    futures = wrenexec.map_sync_with_rate_and_retries(run_command, keylist, rate=10000)

    pywren.wait(futures)
    results = [f.result() for f in futures]
    run_statuses = [f.run_status for f in futures]
    invoke_statuses = [f.invoke_status for f in futures]
    print("read "+ str(results))
    res = {'results' : results, 
           'run_statuses' : run_statuses, 
           'invoke_statuses' : invoke_statuses}
    return res


@cli.command('write')
@click.option('--bucket_name', help='bucket to save files in')
@click.option('--mb_per_file', help='MB of each object in S3', type=int)
@click.option('--number', help='number of files', type=int)
@click.option('--key_prefix', default='', help='S3 key prefix')
@click.option('--outfile', default='s3_benchmark.write.output.pickle', 
              help='filename to save results in')
@click.option('--region', default='us-west-2', help="AWS Region")
def write_command(bucket_name, mb_per_file, number, key_prefix, region, outfile):
    res = write(bucket_name, mb_per_file, number, key_prefix, region)
    pickle.dump(res, open(outfile, 'wb'))

@cli.command('read')
@click.option('--key_file', default=None, help="filename generated by write command, which contains the keys to read")
@click.option('--number', help='number of objects to read, 0 for all', type=int, default=0)
@click.option('--outfile', default='s3_benchmark.read.output.pickle', 
              help='filename to save results in')
@click.option('--read_times', default=1, help="number of times to read each s3 key")
@click.option('--region', default='us-west-2', help="AWS Region")
def read_command(key_file, number, outfile, 
                 read_times, region):

    d = pickle.load(open(key_file, 'rb'))
    bucket_name = d['bucket_name']
    keynames = d['keynames']
    res = read(bucket_name, number, keynames, 
               read_times, region)
    pickle.dump(res, open(outfile, 'wb'))

if __name__ == '__main__':
    cli()
